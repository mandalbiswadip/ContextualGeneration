{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c528bbdf",
   "metadata": {},
   "source": [
    "<h1><center>Inference for paragraph generation</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9278d8",
   "metadata": {},
   "source": [
    "This notebook is used for generating paragraphs (not spans) in a `Dominant only` and `No Introduction` setup. The rouge scores are estimated after extracting the spans from the paragraphs generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4efa135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8345bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ea6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inspect\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29711aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9ffd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18f7e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571cef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.led.modeling_led import LEDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1453cfe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CitationTextGenerationDataset' from 'dataset' (../scripts/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-97c1700e24ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCitationTextGenerationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCitationParagraphGenerationDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CitationTextGenerationDataset' from 'dataset' (../scripts/dataset.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from dataset import CitationTextGenerationDataset, CitationParagraphGenerationDataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f157cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246683d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 16384\n",
    "max_output_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb58d6",
   "metadata": {},
   "source": [
    "### Generation/Inference on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b4039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch, special_tokens=['[Dominant]', '[Reference]']):\n",
    "    # tokenize the inputs and labels\n",
    "    \n",
    "    additional_special_tokens_lookup = {token: idx for token, idx in zip(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)}\n",
    "    special_token_ids = set([additional_special_tokens_lookup[token] for token in special_tokens])\n",
    "    special_token_ids.add(tokenizer.mask_token_id)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        batch[\"source\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    for i_batch in range(len(batch[\"input_ids\"])):\n",
    "        for i_token in range(len(batch[\"input_ids\"][0])):\n",
    "            if batch[\"input_ids\"][i_batch][i_token] in special_token_ids:\n",
    "                batch[\"global_attention_mask\"][i_batch][i_token] = 1\n",
    "            \n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a1137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/bxm200000/models/dominant_only/led_generations/par_v1_cdlm/checkpoint-60500/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b5d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "special_tokens = ['<doc>','</doc>', '[BOS]', '[Dominant]', '[Reference]', '[B_Dominant]',  '[E_Dominant]', '[B_Reference]', '[E_Reference]', '<context>', '</context>']\n",
    "additional_special_tokens = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b09a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b28cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.length_penalty = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707beedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGeneration(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50276, 768, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50276, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50276, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50276, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device).half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87672116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:08<00:00, 40.58it/s]\n"
     ]
    }
   ],
   "source": [
    "val_set = CitationParagraphGenerationDataset(\n",
    "    \"/home/data/XiangciLi/CORWA/annotated_test/\", \n",
    "    tokenizer, \n",
    "    MAX_SENT_LEN = max_input_length,\n",
    "    related_work_path='/home/data/XiangciLi/20200705v1/acl/selected_related_work.jsonl',\n",
    "    cited_metadata_path='/home/data/XiangciLi/20200705v1/acl/selected_cited_metadata.jsonl',\n",
    "    cited_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_cited_pdf_parses.jsonl\",\n",
    "    citing_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_pdf_parses.jsonl\",\n",
    "    skip_no_citations = True,\n",
    "    context_sep_flag=True,\n",
    "    include_intro=False,\n",
    "    add_same_ciation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fa82b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.filter_citation_type(citation_type=\"Dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13537799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ee0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_source(source):\n",
    "    return related_work + \"\\n\\n\" + source.split(\"\\n\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f65df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dt in val_set:\n",
    "#     dt[\"source\"] = get_new_source(dt[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d7d961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(batch, model):\n",
    "    processed_batch = process_data_to_model_inputs(batch, special_tokens=['[Dominant]', '[Reference]'])\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "    predicted_abstract_ids = model.generate(\n",
    "        processed_batch_cuda[\"input_ids\"], \n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"], \n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"]\n",
    "    )\n",
    "    out = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=False)\n",
    "    target = batch[\"target\"]\n",
    "    return out, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089b03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(src):\n",
    "    \"\"\"Get citations given source content\"\"\"\n",
    "    all_citations = []\n",
    "    for cite_data in src.split(\"[B_Reference]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "\n",
    "    for cite_data in src.split(\"[B_Dominant]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "    \n",
    "    return all_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b191f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_data = []\n",
    "\n",
    "#reference_predicted = []\n",
    "#reference_reference = []\n",
    "#dominant_predicted = []\n",
    "#dominant_reference = []\n",
    "for batch in tqdm(DataLoader(val_set, batch_size = 1, shuffle=False)):\n",
    "    processed_batch = process_data_to_model_inputs(batch, special_tokens=['[Dominant]', '[Reference]'])\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "    predicted_abstract_ids = model.generate(\n",
    "        processed_batch_cuda[\"input_ids\"], \n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"], \n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"]\n",
    "    )\n",
    "    out = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=False)\n",
    "#     print(out)\n",
    "    target = batch[\"target\"]\n",
    "    for o, t, part_id, source in zip(out, target, batch[\"id\"], batch[\"source\"]):\n",
    "        accumulated_data.append(\n",
    "            {\"source\": source, \"target\": t, \n",
    "             \"generated\": o, \"part_id\": part_id}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e683c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_utils.write_json(\n",
    "#     accumulated_data, \n",
    "#     os.path.abspath(os.path.join(path, \"../sample_output.json\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a163fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/bxm200000/models/dominant_only/led_generations/span_v1_cdlm/checkpoint-67500/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0324432",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_data = analysis_utils.load_json(\n",
    "    os.path.abspath(os.path.join(path, \"../sample_output.json\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14fd07",
   "metadata": {},
   "source": [
    "### Post-Process paragraph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5cb57b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "def fuzzy_replace(pattern, repl, string):\n",
    "    l = len(pattern.split()) # Length to read orig_str chunk by chunk\n",
    "    splitted = string.split()\n",
    "    for i in range(len(splitted)-l+1):\n",
    "        for k in range(3):\n",
    "            test = \" \".join(splitted[max(0, i - k) : min(len(splitted), i+l + k)])\n",
    "            if fuzz.ratio(pattern, test) > 75: #Using fuzzwuzzy library to test ratio\n",
    "                before = \" \".join(splitted[:max(0, i - k)])\n",
    "                after = \" \".join(splitted[min(len(splitted), i+l + k):])\n",
    "                return_string = before+\" \"+repl+\" \"+after\n",
    "                return return_string.strip(), True #Output will be sandwich of these three strings\n",
    "    return string.strip(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "087fa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_context(paragraph):\n",
    "    \"\"\"strip context from paragraph and only return span\"\"\"\n",
    "    return paragraph.split(\"<context>\")[1].split(\"</context>\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "38e07419",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_valid_count = 0\n",
    "citation_valid_count_target = 0\n",
    "total_cites = 0\n",
    "truth, prediction = [], []\n",
    "bad_citations = []\n",
    "for data in accumulated_data:\n",
    "    \n",
    "# if \"<context>\" in data[\"generated\"] and \"</context>\" in data[\"generated\"]:\n",
    "\n",
    "    target = data[\"target\"]\n",
    "    generation = data[\"generated\"]\n",
    "    target = analysis_utils.remove_punctuations_from_citations(target)    \n",
    "    generation = analysis_utils.remove_punctuations_from_citations(generation)    \n",
    "\n",
    "    cite_found = False\n",
    "    citation_marks  = analysis_utils.get_citations(data[\"source\"])\n",
    "    total_cite_marks = len(citation_marks)\n",
    "    counter = 0\n",
    "    for cite in citation_marks:\n",
    "        total_cites += 1\n",
    "        cite = analysis_utils.remove_punctuations_from_citations(cite)\n",
    "\n",
    "        if cite in target:\n",
    "            #         if found:\n",
    "            target = target.replace(cite, \"\")\n",
    "            citation_valid_count_target += 1\n",
    "#             else:\n",
    "#                 target, found = analysis_utils.fuzzy_replace(cite, \"\", target)\n",
    "\n",
    "        if cite in generation:\n",
    "#         if found:\n",
    "            generation = generation.replace(cite, \"\")\n",
    "            cite_found = True\n",
    "            counter += 1\n",
    "            citation_valid_count += 1\n",
    "        else:\n",
    "            bad_citations.append((cite, generation))\n",
    "            generation, found = fuzzy_replace(cite, \"\", generation)\n",
    "            if found:\n",
    "                citation_valid_count += 1\n",
    "\n",
    "#         target = target.replace(cite, \"\")\n",
    "#         generation = generation.replace(cite, \"\")\n",
    "\n",
    "#         if cite_found and counter == total_cite_marks:\n",
    "\n",
    "    truth.append(target)\n",
    "    prediction.append(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fed52c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1265, 1503, 1538)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_valid_count, citation_valid_count_target, total_cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3e8b1311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2630729884201392, recall=0.28429534882319013, fmeasure=0.24741453254908688), mid=Score(precision=0.2714446877434547, recall=0.29324796387017177, fmeasure=0.25448054483797333), high=Score(precision=0.27975659178423223, recall=0.3021206090975524, fmeasure=0.26116541210016003)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.06270512435123805, recall=0.06899337951986224, fmeasure=0.059071821587468286), mid=Score(precision=0.0676382120896212, recall=0.0744553134507854, fmeasure=0.06349954990113339), high=Score(precision=0.07297767247352686, recall=0.080476118053848, fmeasure=0.06842291602236568)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.19818478187526314, recall=0.21697656151708486, fmeasure=0.18776953651489756), mid=Score(precision=0.20512478708921505, recall=0.22471123315113284, fmeasure=0.1935324190802918), high=Score(precision=0.21226149967637012, recall=0.23222270559398767, fmeasure=0.1997092780503558))}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(\n",
    "    predictions=prediction, \n",
    "    references=truth, \n",
    "    rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"],           \n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1fe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a93b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accumulated_data = deepcopy(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "715755f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, references = [], []\n",
    "dominant_predicted, dominant_reference = [], []\n",
    "reference_predicted, reference_reference = [], []\n",
    "for index, data in enumerate(new_accumulated_data):\n",
    "    target = data[\"target\"]\n",
    "    gen = data[\"generated\"]\n",
    "    \n",
    "    \n",
    "    if \"<context>\" in gen and \"</context>\" in gen:\n",
    "\n",
    "        target = strip_context(data[\"target\"])\n",
    "        gen = strip_context(data[\"generated\"])\n",
    "        data[\"generated\"] = gen\n",
    "        data[\"target\"] = target\n",
    "\n",
    "        new_accumulated_data[index] = data\n",
    "        # remove citations         \n",
    "        for c in get_citations(data[\"source\"]):\n",
    "            c = c.replace(\",\", \"\").replace(\".\", \"\")\n",
    "            gen = gen.replace(\",\", \"\").replace(\".\", \"\").replace(c, \"\")\n",
    "            target = target.replace(\",\", \"\").replace(\".\", \"\").replace(c, \"\")\n",
    "\n",
    "        predicted.append(gen)\n",
    "        references.append(target)\n",
    "\n",
    "        if \"[Dominant]\" in data[\"source\"]:\n",
    "            dominant_predicted.append(gen)\n",
    "            dominant_reference.append(target)\n",
    "\n",
    "\n",
    "        if \"[Reference]\" in data[\"source\"]:\n",
    "            reference_predicted.append(gen)\n",
    "            reference_reference.append(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "294a32c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dominant_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95019184",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_data = [x for x in accumulated_data if \"<context>\" not in x[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5bd7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [analysis_utils.remove_citation_from_sentence(\n",
    "    x[\"generated\"], x[\"source\"]) for x in accumulated_data]\n",
    "\n",
    "references = [analysis_utils.remove_citation_from_sentence(\n",
    "    x[\"target\"], x[\"source\"]) for x in accumulated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0f86c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1271, 1271)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted), len(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e625a",
   "metadata": {},
   "source": [
    "## Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c33509cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91c42f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.24390280193448513, recall=0.2918351856668053, fmeasure=0.24679456410476389), mid=Score(precision=0.2511805527825172, recall=0.29988851296425945, fmeasure=0.2527202804293941), high=Score(precision=0.2595020796084716, recall=0.3084939390424027, fmeasure=0.259299420535417)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.055788757664991466, recall=0.06641996068229464, fmeasure=0.0560660497814295), mid=Score(precision=0.06031282734257353, recall=0.07199184742085965, fmeasure=0.060523176233065454), high=Score(precision=0.0654290394283746, recall=0.07786736270257734, fmeasure=0.06544428245409971)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.18349067504551142, recall=0.22309379254538952, fmeasure=0.18673939053803326), mid=Score(precision=0.18957654595085724, recall=0.2297622750505995, fmeasure=0.19189791523602023), high=Score(precision=0.1956725732882889, recall=0.23735603869967975, fmeasure=0.19742646940949132))}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(\n",
    "    predictions=predicted, \n",
    "    references=references, \n",
    "    rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"],           \n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86692e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_datapoints = deepcopy(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f8aa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthg_targets = np.array([tokenizer.tokenize(x[\"target\"]).__len__() for x in good_datapoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "599b4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_generations = np.array([tokenizer.tokenize(x[\"generated\"]).__len__() for x in good_datapoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cde88479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   1.,   0.,   3.,   3.,  23., 153., 954., 119.,  13.]),\n",
       " array([-365. , -316.1, -267.2, -218.3, -169.4, -120.5,  -71.6,  -22.7,\n",
       "          26.2,   75.1,  124. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lenthg_targets - length_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb0a4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-343862115e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenthg_targets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlength_generations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(lenthg_targets - length_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3d438d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.47836349331235"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(lenthg_targets - length_generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8ab6b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Target length - Generation Length - Paragraph generation')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lenthg_targets - length_generations, bins=100)\n",
    "plt.title(\"Target length - Generation Length - Paragraph generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8604f7",
   "metadata": {},
   "source": [
    "### Run Text Generation on single datapoint - Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48bf35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\"While some work has conducted experiments with different BPE settings, they are generally very limited in the range of configurations explored.\\nFor example, Sennrich et al. (2016) , the original paper that proposed the BPE method, compared the system performance when using 60k separate BPE and 90k joint BPE.\\nThey found 90k to work better and used that for their subsequent winning WMT 2017 new translation shared task submission (Sennrich et al., 2017) .\\nWu et al. (2016) , on the other hand, found 8k-32k merge operations achieving optimal BLEU score performance for the wordpiece method.\\nDenkowski and Neubig (2017) explored several hyperparameter settings, including number of BPE merge operations, to establish strong baseline for NMT on LSTM-based architectures.\\nWhile Denkowski and Neubig (2017) showed that BPE models are clearly better than word-level models, their experiments on 16k and 32k BPE configuration did not show much difference.\\nThey therefore recommended \\\"32K as a generally effective vocabulary size and 16K as a contrastive condition when building systems on less than 1 million parallel sentences\\\".\\nHowever, while studying deep character-based LSTM-based translation models,  [Dominant] \\nRecently, Renduchintala et al. (2018) also showed that it is important to tune the number of BPE merge operations and found no typical optimal BPE configuration for their LSTM-based architecture while sweeping over several language pairs in the low-resource setting.\\nIt should be noticed that the results from the above studies actually contradict with each other, and there is still no clear consensus as to what is the best practice for BPE application.\\nMoreover, all the work surveyed above was done with LSTM-based architectures.\\nTo this day, we are not aware of any work that explored the interaction of BPE with the Transformer architecture.\\n\\n [B_Dominant] Du et al. (2017) </s> Learning to Ask: Neural Question Generation for Reading Comprehension | We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence-vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequenceto-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer). [E_Dominant]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba2e5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Shen et al. (2019) proposed ml-VAE-D with multi-level latent variables.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9254d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation(gen_model, source):\n",
    "    test_data = {\"source\": source, \"target\": \"target 123\", \"id\": \"test_123\"}\n",
    "\n",
    "    test_data_list = [test_data]\n",
    "    for batch in DataLoader(test_data_list, batch_size = 1, shuffle=False):\n",
    "        out, ta = run_model(batch, gen_model)\n",
    "        break\n",
    "    return out[0]\n",
    "#     return strip_context(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c77423d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>While some work has conducted experiments with different BPE settings, they are generally very limited in the range of configurations explored.\\nFor example, Sennrich et al. (2016), the original paper that proposed the BPE method, compared the system performance when using 60k separate BPE and 90k joint BPE.\\nThey found 90k to work better and used that for their subsequent winning WMT 2017 new translation shared task submission (Sennrich Et al., 2017).\\nWu et al., 2016), on the other hand, found 8k-32k merge operations achieving optimal BLEU score performance for the wordpiece method.\\nDenkowski and Neubig (2017) explored several hyperparameter settings, including number of BPE merge operations, to establish strong baseline for NMT on LSTM-based architectures.\\nWhile Denkowski andNeubig the2017) showed that BPE models are clearly better than word-level models, their experiments on 16k and 32k BPE configuration did not show much difference.\\nthey therefore recommended \"32K as a generally effective vocabulary size and 16K as as a contrastive condition when building systems on less than 1 million parallel sentences\".\\nHowever, while studying deep character-based LSTructure-based translation models,  <context> Van den Bosch and Daelemans (2018) found that the performance of the Transformer architecture was significantly worse than that of the word-based model in the low-resource setting.\\nTheir experiments were conducted in the context of question generation, and they concluded that attention-based neural models are not the best choice. </context> \\nRecently, Renduchintala et Al. (2018), also showed that it is important to tune the number of bPE merge Operations and found no typical optimal BPE configurations for their LSTGM-based architecture while sweeping over several language pairs in the Low-resource settings.\\nIt should be noticed that the results from the above studies actually contradict with each other, and there is still no clear consensus as to what is the best practice for BPE application.\\nMoreover, all the work surveyed above was done with LSTOne-based networks.\\nTo this day, we are not aware of any work that explored the interaction of BESA with the Trans transformer architecture.\\n\\n</s>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generation(model, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1344418",
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_paper = \"\"\"[B_Dominant] Michel and Neubig (2018) </s> Extreme Adaptation for Personalized Neural Machine Translation | Every person speaks or writes their own flavor of their native language, influenced by a number of factors: the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the system should perform translation, but this is not captured well by standard one-sizefits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on TED talks in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text. [E_Dominant]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0c34f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(source):\n",
    "    return source.split(\"\\n\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792bfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(val_set), 20):\n",
    "#     ddd = val_set[i]\n",
    "#     source = get_context(ddd[\"source\"]) + \"\\n\\n\" + cited_paper\n",
    "#     try:\n",
    "#         print(get_generation(model, source))\n",
    "#     except:\n",
    "#         pass\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85313d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
