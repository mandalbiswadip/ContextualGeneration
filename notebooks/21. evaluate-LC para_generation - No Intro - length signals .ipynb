{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4efa135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8345bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ea6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29711aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bd9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0681a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HuggingfaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24c1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f7e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f88756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinusoidal_positional_embedding_new import SinusoidalPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1453cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from dataset import (CitationTextGenerationDataset, \n",
    "                     CitationParagraphGenerationDatasetOld, \n",
    "                     CitationParagraphGenerationDataset,\n",
    "                     CitationContextSpanSeparateDataset\n",
    "                    )\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab5594cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis_utils\n",
    "from modelling_led_multitask import (\n",
    "    LEDForConditionalGenerationAddContextLength as LEDForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfaafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 16384\n",
    "max_output_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b4039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch, special_tokens=None, length=None, classification=False):\n",
    "    # tokenize the inputs and labels\n",
    "    if special_tokens is None:\n",
    "        special_tokens = ['[Dominant]', '[Reference]']\n",
    "    \n",
    "    additional_special_tokens_lookup = {token: idx for token, idx in zip(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)}\n",
    "    special_token_ids = set([additional_special_tokens_lookup[token] for token in special_tokens])\n",
    "    special_token_ids.add(tokenizer.mask_token_id)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        batch[\"source\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    if length:\n",
    "        batch[\"length\"] = length\n",
    "    else:\n",
    "        span_outputs = tokenizer(\n",
    "            batch[\"span_target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_output_length,\n",
    "            add_special_tokens=True \n",
    "        )\n",
    "\n",
    "        batch[\"length\"] = [sum(x) for x in span_outputs.attention_mask]\n",
    "\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    for i_batch in range(len(batch[\"input_ids\"])):\n",
    "        for i_token in range(len(batch[\"input_ids\"][0])):\n",
    "            if batch[\"input_ids\"][i_batch][i_token] in special_token_ids:\n",
    "                batch[\"global_attention_mask\"][i_batch][i_token] = 1\n",
    "    \n",
    "    # setting [CLS] token as global attention token for classification\n",
    "    if classification:\n",
    "        batch[\"global_attention_mask\"][i_batch][0] = 1\n",
    "            \n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a1137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/bxm200000/models/dominant_only/length_control/para_cdlm_v1_add_context_length_perturb/checkpoint-36500/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b5d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "special_tokens = ['<doc>','</doc>', '[BOS]', '[Dominant]', '[Reference]', '[B_Dominant]',  '[E_Dominant]', '[B_Reference]', '[E_Reference]', '<context>', '</context>']\n",
    "additional_special_tokens = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40df09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "707beedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9975be88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.add_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17de6a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGenerationAddContextLength(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50276, 768, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50276, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50276, 768, padding_idx=1)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50276, bias=False)\n",
       "  (lm_length): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.led.decoder.embed_positions.sinpostype = None\n",
    "# model.config.sinpostype = None\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87672116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:12<00:00, 29.61it/s]\n"
     ]
    }
   ],
   "source": [
    "val_set = CitationParagraphGenerationDataset(\n",
    "    \"/home/data/XiangciLi/CORWA/annotated_test/\", \n",
    "    tokenizer, \n",
    "    MAX_SENT_LEN = max_input_length,\n",
    "    related_work_path='/home/data/XiangciLi/20200705v1/acl/selected_related_work.jsonl',\n",
    "    cited_metadata_path='/home/data/XiangciLi/20200705v1/acl/selected_cited_metadata.jsonl',\n",
    "    cited_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_cited_pdf_parses.jsonl\",\n",
    "    citing_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_pdf_parses.jsonl\",\n",
    "    skip_no_citations = True,context_sep_flag=True,\n",
    "    include_intro=False,\n",
    "    add_length_signal=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41c663f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.filter_citation_type(citation_type=\"Dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "089b03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(src):\n",
    "    \"\"\"Get citations given source content\"\"\"\n",
    "    all_citations = []\n",
    "    for cite_data in src.split(\"[B_Reference]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "\n",
    "    for cite_data in src.split(\"[B_Dominant]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "    \n",
    "    return all_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd27a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1198 [00:00<?, ?it/s]/home/bxm200000/anaconda3/envs/tagger_4/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/bxm200000/anaconda3/envs/tagger_4/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      " 96%|█████████▌| 1146/1198 [15:52:09<40:51, 47.15s/it]   "
     ]
    }
   ],
   "source": [
    "# accumulated_data = []\n",
    "for batch in tqdm(DataLoader(val_set[124:], batch_size = 1, shuffle=False)):\n",
    "    processed_batch = process_data_to_model_inputs(\n",
    "        batch, \n",
    "        special_tokens=['[Dominant]', '[Reference]'], \n",
    "        classification=True\n",
    "    )\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\", \"length\", \"context_length\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "    model_kwargs = {'decoder_context_length' : processed_batch_cuda[\"context_length\"]}\n",
    "    predicted_abstract_ids = model.generate(\n",
    "        input_ids = processed_batch_cuda[\"input_ids\"], \n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"], \n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"],\n",
    "        **model_kwargs\n",
    "    )\n",
    "    out = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=False)\n",
    "    target = batch[\"target\"]\n",
    "    for o, t, part_id, source in zip(out, target, batch[\"id\"], batch[\"source\"]):\n",
    "        accumulated_data.append(\n",
    "            {\"source\": source, \"target\": t, \n",
    "             \"generated\": o, \"part_id\": part_id}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0db27709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63eb20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffs = []\n",
    "\n",
    "# for dt in accumulated_data:\n",
    "#     diffs.append(tokenizer.tokenize(dt[\"target\"]).__len__() - tokenizer.tokenize(dt[\"generated\"]).__len__() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ed80788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95ee6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(tensor):\n",
    "    return tensor.cpu().detach().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c59125b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1322/1322 [12:43<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# LENGTH PREDICTION\n",
    "length_data = []\n",
    "for batch in tqdm(DataLoader(val_set, batch_size = 1, shuffle=False)):\n",
    "    processed_batch = process_data_to_model_inputs(\n",
    "        batch, \n",
    "        special_tokens=None, \n",
    "        classification=True\n",
    "    )\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\", \"length\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "    model_kwargs = {'decoder_length' : processed_batch_cuda[\"length\"].unsqueeze(0)}\n",
    "\n",
    "    \n",
    "    cls_encoder = model.led.encoder(\n",
    "        input_ids=processed_batch_cuda[\"input_ids\"],\n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"],\n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"],\n",
    "        return_dict=True\n",
    "    )[0][:,0,:]\n",
    "    \n",
    "    if model.config.predict_log_length:\n",
    "        length_pred = torch.exp(model.lm_length(cls_encoder))\n",
    "    else:\n",
    "        length_pred = model.lm_length(cls_encoder)\n",
    "    length_pred = length_pred.squeeze(-1)    \n",
    "    \n",
    "    for l_pred, l_actual in zip(length_pred, processed_batch_cuda[\"length\"]):\n",
    "        length_data.append( (get_val(l_pred), get_val(l_actual)) )\n",
    "        \n",
    "    \n",
    "#     for l_pred, l_actual in zip(length_pred, processed_batch_cuda[\"length\"]):\n",
    "#         length_data.append( (get_val(l_pred), get_val(l_actual)) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8af36531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(length_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ef88e",
   "metadata": {},
   "source": [
    "### Save Length predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31567d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_len_data = []\n",
    "for dt, l in zip(val_set, length_data):\n",
    "\n",
    "    store_len_data.append({\"id\": dt[\"id\"], \"length\": (float(l[0]), float(l[1])) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8777daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.abspath(os.path.join(path, \"../length_output.json\")), 'w') as f:\n",
    "#     json.dump(store_len_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dde504",
   "metadata": {},
   "source": [
    "### Load Length Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "603cc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(path, \"../length_output.json\")), 'r') as f:\n",
    "    store_len_data = json.loads(f.read())\n",
    "length_data = [(*x[\"length\"], ) for x in store_len_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828454e2",
   "metadata": {},
   "source": [
    "### Save Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "092bbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ba8cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_tokens = [\"<s>\", \"</s>\"]\n",
    "for index, data in enumerate(accumulated_data):\n",
    "    for token in sep_tokens:\n",
    "        data[\"generated\"] = data[\"generated\"].replace(token, \"\").strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74ab3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(path, \"../sample_para_output.json\")), 'w') as f:\n",
    "    json.dump(accumulated_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907f1e6",
   "metadata": {},
   "source": [
    "### Load generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a34f033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(path, \"../sample_para_output.json\")), 'r') as f:\n",
    "    accumulated_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b4e4a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7e35291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_predictions = np.array(list(zip(*length_data))[0])\n",
    "lenthg_targets = np.array(list(zip(*length_data))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f4515438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b50c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b61e1a78",
   "metadata": {},
   "source": [
    "### Generated vs Target length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c3fd01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_datapoints = analysis_utils.get_valid_paragraph_datapoints(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fca72c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6f8aa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthg_targets = np.array([tokenizer.tokenize(x[\"target\"]).__len__() for x in good_datapoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "599b4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_generations = np.array([tokenizer.tokenize(x[\"generated\"]).__len__() for x in good_datapoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "40b222d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.9415337889142"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs( lenthg_targets - length_generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "07016b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.305998481397115"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( lenthg_targets - length_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0ec51a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.84516161077924"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(lenthg_targets, length_generations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0df2b833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Target length - Generation Length - Paragraph generation LC')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lenthg_targets - length_generations, bins=100, range = (-150, 150))\n",
    "plt.title(\"Target length - Generation Length - Paragraph generation LC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10449a",
   "metadata": {},
   "source": [
    "### Generation vs Predicted Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b55ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f14fd07",
   "metadata": {},
   "source": [
    "### Post-Process paragraph Generation - extract spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "087fa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_context(paragraph):\n",
    "    \"\"\"strip context from paragraph and only return span\"\"\"\n",
    "    return paragraph.split(\"<context>\")[1].split(\"</context>\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc7087bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a93b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accumulated_data = deepcopy(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acd79065",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "715755f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, references = [], []\n",
    "dominant_predicted, dominant_reference = [], []\n",
    "reference_predicted, reference_reference = [], []\n",
    "for index, data in enumerate(new_accumulated_data):\n",
    "    target = data[\"target\"]\n",
    "    gen = data[\"generated\"]\n",
    "    \n",
    "    \n",
    "    if \"<context>\" in gen and \"</context>\" in gen:\n",
    "\n",
    "        target = strip_context(data[\"target\"])\n",
    "        gen = strip_context(data[\"generated\"])\n",
    "        data[\"generated\"] = gen\n",
    "        data[\"target\"] = target\n",
    "\n",
    "        new_accumulated_data[index] = data\n",
    "        # remove citations         \n",
    "        for c in get_citations(data[\"source\"]):\n",
    "            c = c.replace(\",\", \"\").replace(\".\", \"\")\n",
    "            gen = gen.replace(\",\", \"\").replace(\".\", \"\").replace(c, \"\")\n",
    "            target = target.replace(\",\", \"\").replace(\".\", \"\").replace(c, \"\")\n",
    "\n",
    "        predicted.append(gen)\n",
    "        references.append(target)\n",
    "\n",
    "        if \"[Dominant]\" in data[\"source\"]:\n",
    "            dominant_predicted.append(gen)\n",
    "            dominant_reference.append(target)\n",
    "\n",
    "\n",
    "        if \"[Reference]\" in data[\"source\"]:\n",
    "            reference_predicted.append(gen)\n",
    "            reference_reference.append(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d2b5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dominant_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "294a32c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dominant_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91c42f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.252551821542718, recall=0.26237485657003834, fmeasure=0.23625074011341463), mid=Score(precision=0.2603201468414728, recall=0.27014311922942136, fmeasure=0.24255346907640699), high=Score(precision=0.2687455075158768, recall=0.2780549912317007, fmeasure=0.24889547415893087)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.05404289084709848, recall=0.05761130475788302, fmeasure=0.05089834534993279), mid=Score(precision=0.05937584493041693, recall=0.062283362650859805, fmeasure=0.055127268685939146), high=Score(precision=0.06396616794563971, recall=0.067196774794971, fmeasure=0.059183562033996384)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.19166101346981235, recall=0.2009076742426963, fmeasure=0.1797225313079823), mid=Score(precision=0.19817983182594884, recall=0.20730878197066505, fmeasure=0.18501425713017183), high=Score(precision=0.20465630695008863, recall=0.21403363459838645, fmeasure=0.1896533675600201))}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(\n",
    "    predictions=predicted, \n",
    "    references=references, \n",
    "    rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"],           \n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e4c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
